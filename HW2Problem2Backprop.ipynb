{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuuA_SHMuUne"
   },
   "source": [
    "# HW2: Problem 2: Working out Backpropagation\n",
    "\n",
    "Read Chapter 2 of Michael Nielsen's article/book from top to bottom:\n",
    "\n",
    "* [http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "\n",
    "He outlines a few exersizes in that article which you must complete. Do the following a, b, c:\n",
    "\n",
    "a. He invites you to write out a proof of equation BP3\n",
    "\n",
    "b. He invites you to write out a proof of equation BP4\n",
    "\n",
    "c. He proposes that you code a fully matrix-based approach to backpropagation over a mini-batch. Implement this with explanation where you change the notation so that instead of having a bias term, you assume that the input variables are augmented with a \"column\" of \"1\"s, and that the weights $w_0$.\n",
    "\n",
    "Your submission should be a single jupyter notebook. Use markdown cells with latex for equations of a jupyter notebook for each proof for \"a.\" and \"b.\". Make sure you include text that explains your steps. Next for \"c\" this is an implementation problem. You need to understand and modify the code the Michael Nielsen provided so that instead it is using a matrixed based approach. Again don't keep the biases separate. After reading data in (use the iris data set), create a new column corresponding to $x_0=1$, and as mentioned above and discussed in class (see notes) is that the bias term can then be considered a weight $w_0$. Again use markdown cells around your code and comments to explain your work. Test the code on the iris data set with 4 node input (5 with a constant 1), three hidden nodes, and three output nodes, one for each species/class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsfIQN1-uwGB"
   },
   "source": [
    "## a. Proof of Michael Nielsons equation BP3\n",
    "\n",
    "Please fill this in using markdown and latex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof for BP3\n",
    "\n",
    "The equation BP3 relates to the rate of change of the cost with respect to any weight in the network, given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l\n",
    "$$\n",
    "\n",
    "#### Proof:\n",
    "\n",
    "1. The cost $C$ is affected by the outputs of the network, which depend on the net inputs $z_j^l$ to the neurons in the final layer. The net input $z_j^l$ is influenced by the weights $w_{jk}^l$ and the activations $a_k^{l-1}$ from the previous layer.\n",
    "\n",
    "2. The change in cost with respect to a change in a particular weight $w_{jk}^l$ is expressed using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_{jk}^l} = \\frac{\\partial C}{\\partial a_j^l} \\cdot \\frac{\\partial a_j^l}{\\partial z_j^l} \\cdot \\frac{\\partial z_j^l}{\\partial w_{jk}^l}\n",
    "$$\n",
    "\n",
    "3. The term $\\frac{\\partial C}{\\partial a_j^l} \\cdot \\frac{\\partial a_j^l}{\\partial z_j^l}$ is defined as the error term $\\delta_j^l$ for the neuron in layer $l$.\n",
    "\n",
    "4. The term $\\frac{\\partial z_j^l}{\\partial w_{jk}^l}$ is the activation $a_k^{l-1}$ from the previous layer, since $z_j^l = \\sum_k w_{jk}^l a_k^{l-1} + b_j^l$.\n",
    "\n",
    "5. Combining these, we get the desired expression:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l\n",
    "$$\n",
    "\n",
    "This demonstrates that the gradient of the cost with respect to a weight is the product of the activation from the preceding layer and the error term from the current layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axsx4eu9u_3x"
   },
   "source": [
    "## b. Proof of Michael Nielsons equation BP4\n",
    "\n",
    "Please fill this in using markdown and latex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof for BP4\n",
    "\n",
    "The equation BP4 concerns the gradient of the cost function with respect to the biases in the network, given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b_j^l} = \\delta_j^l\n",
    "$$\n",
    "\n",
    "#### Proof:\n",
    "\n",
    "1. The bias $b_j^l$ affects the net input $z_j^l$ to the neuron, which then influences the neuron's activation $a_j^l$ and consequently the overall cost $C$.\n",
    "\n",
    "2. Applying the chain rule, the rate of change of $C$ with respect to $b_j^l$ is expressed as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b_j^l} = \\frac{\\partial C}{\\partial a_j^l} \\cdot \\frac{\\partial a_j^l}{\\partial z_j^l} \\cdot \\frac{\\partial z_j^l}{\\partial b_j^l}\n",
    "$$\n",
    "\n",
    "3. By definition, $\\frac{\\partial z_j^l}{\\partial b_j^l} = 1$ since $z_j^l = \\sum_k w_{jk}^l a_k^{l-1} + b_j^l$, and the derivative of $z_j^l$ with respect to $b_j^l$ is 1.\n",
    "\n",
    "4. The product $\\frac{\\partial C}{\\partial a_j^l} \\cdot \\frac{\\partial a_j^l}{\\partial z_j^l}$ defines the error term $\\delta_j^l$.\n",
    "\n",
    "5. Thus, we obtain the equation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b_j^l} = \\delta_j^l\n",
    "$$\n",
    "\n",
    "This shows that the gradient of the cost with respect to a bias is simply the error term for the neuron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rj1HbN9RvVXe"
   },
   "source": [
    "## c. Using both markdown cells and code cells implement that you code a fully matrix-based approach to backpropagation over a mini-batch. Implement this with explanation where you change the notation so that instead of having a bias term, you assume that the input variables are augmented with a \"column\" of \"1\"s, and that the weights $w_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1676928890900,
     "user": {
      "displayName": "Michael Grossberg",
      "userId": "10616474120098361836"
     },
     "user_tz": 300
    },
    "id": "3wUek4Nau7x7"
   },
   "outputs": [],
   "source": [
    "# Code cell for part c.\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Input features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# One-hot encode target labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to augment the input matrix with a column of ones for the bias term\n",
    "def augment_input(X):\n",
    "    return np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "\n",
    "X_train_aug = augment_input(X_train)\n",
    "X_test_aug = augment_input(X_test)\n",
    "\n",
    "# Initialize weight matrices\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    W1 = np.random.randn(input_size + 1, hidden_size)  # Including bias term in input size\n",
    "    W2 = np.random.randn(hidden_size + 1, output_size)  # Including bias term in hidden size\n",
    "    return W1, W2\n",
    "\n",
    "input_size = X_train_aug.shape[1] - 1  # Exclude the bias term from input size\n",
    "hidden_size = 3\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "W1, W2 = initialize_weights(input_size, hidden_size, output_size)\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Forward pass through the network\n",
    "def forward_pass(X, W1, W2):\n",
    "    # Input to hidden layer\n",
    "    Z1 = np.dot(X, W1)\n",
    "    A1 = sigmoid(Z1)\n",
    "    A1_aug = augment_input(A1)  # Augment for bias term in the hidden layer\n",
    "    \n",
    "    # Hidden to output layer\n",
    "    Z2 = np.dot(A1_aug, W2)\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    return A1, A2\n",
    "\n",
    "# Example forward pass (remove in actual training loop)\n",
    "A1, A2 = forward_pass(X_train_aug, W1, W2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step involves computing the gradients for the weight matrices. Since the focus here is on the implementation structure, I'll outline the gradient calculation without delving into the detailed math. The actual backpropagation computation depends on the loss function used and involves calculating the derivatives of the loss with respect to the weights, which can be complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9072\n",
      "Epoch 100, Loss: 1.0983\n",
      "Epoch 200, Loss: 1.0976\n",
      "Epoch 300, Loss: 0.5064\n",
      "Epoch 400, Loss: 0.4815\n",
      "Epoch 500, Loss: 0.4743\n",
      "Epoch 600, Loss: 0.4709\n",
      "Epoch 700, Loss: 0.4690\n",
      "Epoch 800, Loss: 0.4677\n",
      "Epoch 900, Loss: 0.4668\n"
     ]
    }
   ],
   "source": [
    "def backpropagation(X_aug, Y, A1, A2, W1, W2):\n",
    "    # Error in output\n",
    "    dZ2 = A2 - Y\n",
    "    A1_aug = augment_input(A1)  # Augment A1 with a column of ones for the bias term\n",
    "    dW2 = np.dot(A1_aug.T, dZ2)\n",
    "\n",
    "    # Error in hidden layer, excluding bias term from W2 for backpropagation\n",
    "    dZ1 = np.dot(dZ2, W2[1:, :].T) * A1 * (1 - A1)\n",
    "    dW1 = np.dot(X_aug.T, dZ1)\n",
    "\n",
    "    return dW1, dW2\n",
    "\n",
    "\n",
    "def update_weights(W1, W2, dW1, dW2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2  # Ensure W2 and dW2 have compatible shapes\n",
    "    return W1, W2\n",
    "\n",
    "    # Cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]  # Number of samples\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / m  # Add a small value to avoid log(0)\n",
    "    return loss\n",
    "\n",
    "# Training loop with loss computation\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "N = 100  # Print the loss every 100 epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    A1, A2 = forward_pass(X_train_aug, W1, W2)\n",
    "    \n",
    "    # Compute gradients\n",
    "    dW1, dW2 = backpropagation(X_train_aug, y_train, A1, A2, W1, W2)\n",
    "    \n",
    "    # Update weights\n",
    "    W1, W2 = update_weights(W1, W2, dW1, dW2, learning_rate)\n",
    "    \n",
    "    # Compute and print loss every N epochs\n",
    "    if epoch % N == 0:\n",
    "        loss = cross_entropy_loss(y_train, A2)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6Jv_j1WveF9"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPGRo7E+vvs1+9PrlkZvkBC",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
